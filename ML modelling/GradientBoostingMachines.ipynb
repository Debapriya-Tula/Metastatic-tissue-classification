{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f33060",
   "metadata": {},
   "source": [
    "### Extract and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defd2be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 11:03:00.335586: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-10 11:03:00.571149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733857380.656871    9763 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733857380.675120    9763 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 11:03:00.807926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from skimage.exposure import match_histograms\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c42257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Labels', 'Metadata', 'camelyonpatch_level_2_split_train_mask', 'pcam']\n"
     ]
    }
   ],
   "source": [
    "data_path = '../Data/metastatic-tissue-classification-patchcamelyon'\n",
    "\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "pcam_path = data_path + '/pcam'\n",
    "pcam_training_file = os.path.join(pcam_path, \"training_split.h5\")\n",
    "pcam_validation_file = os.path.join(pcam_path, \"validation_split.h5\")\n",
    "pcam_test_file = os.path.join(pcam_path, \"test_split.h5\")\n",
    "\n",
    "with h5py.File(pcam_training_file, 'r') as f:\n",
    "  training_data = f['x'][:]\n",
    "\n",
    "with h5py.File(pcam_validation_file, 'r') as f:\n",
    "  val_data = f['x'][:]\n",
    "\n",
    "with h5py.File(pcam_test_file, 'r') as f:\n",
    "  test_data = f['x'][:]\n",
    "\n",
    "# Labels\n",
    "labels_path = data_path + '/Labels/Labels'\n",
    "label_training_file = os.path.join(labels_path, \"camelyonpatch_level_2_split_train_y.h5\")\n",
    "label_validation_file = os.path.join(labels_path, \"camelyonpatch_level_2_split_valid_y.h5\")\n",
    "label_test_file = os.path.join(labels_path, \"camelyonpatch_level_2_split_test_y.h5\")\n",
    "\n",
    "with h5py.File(label_training_file, 'r') as f:\n",
    "  training_labels = f['y'][:]\n",
    "\n",
    "with h5py.File(label_validation_file, 'r') as f:\n",
    "  val_labels = f['y'][:]\n",
    "\n",
    "with h5py.File(label_test_file, 'r') as f:\n",
    "  test_labels = f['y'][:]\n",
    "\n",
    "# Metadata\n",
    "metadata_path = data_path + '/Metadata/Metadata/'\n",
    "\n",
    "training_metadata = pd.read_csv(metadata_path + 'train_metadata.csv')\n",
    "val_metadata = pd.read_csv(metadata_path + 'valid_metadata.csv')\n",
    "test_metadata = pd.read_csv(metadata_path + 'test_metadata.csv')\n",
    "\n",
    "reference_image = training_data[176298]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca3983",
   "metadata": {},
   "source": [
    "### Use Random Subset of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d6477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Training Data Shape: (2621, 96, 96, 3)\n",
      "Sampled Training Labels Shape: (2621, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed for reproducibility\n",
    "random_seed = 1\n",
    "\n",
    "# Define the sampling fraction \n",
    "sampling_fraction = 0.01\n",
    "\n",
    "# Randomly sample indices\n",
    "num_samples = int(training_data.shape[0] * sampling_fraction)\n",
    "random_indices = np.random.choice(training_data.shape[0], num_samples, replace=False)\n",
    "\n",
    "# Sample the data and labels\n",
    "training_data_sampled = training_data[random_indices]\n",
    "training_labels_sampled = training_labels[random_indices]\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Sampled Training Data Shape: {training_data_sampled.shape}\")\n",
    "print(f\"Sampled Training Labels Shape: {training_labels_sampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "044816a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing Functions\n",
    "def normalize_pixel_values(image):\n",
    "    \"\"\"Normalize pixel values to [0, 1].\"\"\"\n",
    "    return image / 255.0\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Apply preprocessing to a single image.\"\"\"\n",
    "    image = normalize_pixel_values(image)  # Normalize pixel values\n",
    "    return image\n",
    "\n",
    "# Preprocess Images\n",
    "print(\"Preprocessing training data...\")\n",
    "training_data_preprocessed = np.array([preprocess_image(img) for img in training_data_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59dadf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/d/dfang/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/u/home/d/dfang/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation data...\n",
      "Extracting features from validation data...\n"
     ]
    }
   ],
   "source": [
    "def extract_features(images):\n",
    "    \"\"\"Extract features from images using ResNet-50.\"\"\"\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()  # Remove the classification head\n",
    "    model.eval()\n",
    "\n",
    "    features = []\n",
    "    preprocess_pipeline = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img in images:\n",
    "            img = preprocess_pipeline(img).unsqueeze(0).float()  # Add batch dimension and convert to float32\n",
    "            feature = model(img)\n",
    "            features.append(feature.squeeze().numpy())  # Convert to numpy array\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Extracting features from training data...\")\n",
    "training_features = extract_features(training_data_preprocessed)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "validation_data_preprocessed = np.array([preprocess_image(img) for img in val_data])\n",
    "\n",
    "print(\"Extracting features from validation data...\")\n",
    "validation_features = extract_features(validation_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba7127f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping features...\n",
      "Training Gradient Boosting Machine...\n"
     ]
    }
   ],
   "source": [
    "# Reshape features to 2D for compatibility with GradientBoostingClassifier\n",
    "print(\"Reshaping features...\")\n",
    "training_features = training_features.reshape(training_features.shape[0], -1)\n",
    "validation_features = validation_features.reshape(validation_features.shape[0], -1)\n",
    "\n",
    "# Flatten labels to 1D\n",
    "training_labels = training_labels.flatten()\n",
    "validation_labels = val_labels.flatten()\n",
    "\n",
    "# Train Gradient Boosting Machine\n",
    "print(\"Training Gradient Boosting Machine...\")\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d28445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping features and labels...\n",
      "Evaluating on validation data...\n",
      "Validation Accuracy: 0.8143\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82     16399\n",
      "           1       0.82      0.80      0.81     16369\n",
      "\n",
      "    accuracy                           0.81     32768\n",
      "   macro avg       0.81      0.81      0.81     32768\n",
      "weighted avg       0.81      0.81      0.81     32768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure proper reshaping for training features and labels\n",
    "print(\"Reshaping features and labels...\")\n",
    "training_features = training_features.reshape(training_features.shape[0], -1)\n",
    "validation_features = validation_features.reshape(validation_features.shape[0], -1)\n",
    "\n",
    "# Flatten the labels to 1D\n",
    "training_labels_sampled = training_labels_sampled.flatten()\n",
    "validation_labels = validation_labels.flatten()\n",
    "\n",
    "gbm.fit(training_features, training_labels_sampled)\n",
    "\n",
    "# Predict on Validation Data\n",
    "print(\"Evaluating on validation data...\")\n",
    "validation_preds = gbm.predict(validation_features)\n",
    "accuracy = accuracy_score(validation_labels, validation_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(validation_labels, validation_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff130f1b",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machine w/ Reinhard Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3126c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchstain\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def apply_reinhard_normalization(image, reference_image):\n",
    "    \"\"\"\n",
    "    Apply Reinhard normalization to a single image with error handling.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.ndarray): The image to normalize, shape (H, W, C) in RGB format.\n",
    "        reference_image (np.ndarray): The reference image for normalization, shape (H, W, C) in RGB format.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The normalized image, shape (H, W, C) in normalized format.\n",
    "        None: If normalization fails for any reason.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the ReinhardNormalizer\n",
    "        normalizer = torchstain.normalizers.ReinhardNormalizer()\n",
    "\n",
    "        # Fit the normalizer with the reference image\n",
    "        normalizer.fit(reference_image)\n",
    "\n",
    "        # Normalize the image\n",
    "        normalized_image = normalizer.normalize(image)\n",
    "\n",
    "        # Return the normalized image\n",
    "        return normalized_image\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during Reinhard normalization: {e}\")\n",
    "\n",
    "    # Return None if normalization fails\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6de536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing Functions\n",
    "def normalize_pixel_values(image):\n",
    "    \"\"\"Normalize pixel values to [0, 1].\"\"\"\n",
    "    return image / 255.0\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Apply preprocessing to a single image.\"\"\"\n",
    "    image = apply_reinhard_normalization(image, reference_image) # Stain normalization\n",
    "    image = normalize_pixel_values(image)  # Normalize pixel values\n",
    "    return image\n",
    "\n",
    "# Preprocess Images\n",
    "print(\"Preprocessing training data...\")\n",
    "training_data_preprocessed = np.array([preprocess_image(img) for img in training_data_sampled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0486e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from training data...\n",
      "Preprocessing validation data...\n",
      "Extracting features from validation data...\n"
     ]
    }
   ],
   "source": [
    "def extract_features(images):\n",
    "    \"\"\"Extract features from images using ResNet-50.\"\"\"\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()  # Remove the classification head\n",
    "    model.eval()\n",
    "\n",
    "    features = []\n",
    "    preprocess_pipeline = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img in images:\n",
    "            img = preprocess_pipeline(img).unsqueeze(0).float()  # Add batch dimension and convert to float32\n",
    "            feature = model(img)\n",
    "            features.append(feature.squeeze().numpy())  # Convert to numpy array\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Extracting features from training data...\")\n",
    "training_features = extract_features(training_data_preprocessed)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "validation_data_preprocessed = np.array([preprocess_image(img) for img in val_data])\n",
    "\n",
    "print(\"Extracting features from validation data...\")\n",
    "validation_features = extract_features(validation_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10a984fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping features...\n",
      "Training Gradient Boosting Machine...\n"
     ]
    }
   ],
   "source": [
    "# Reshape features to 2D for compatibility with GradientBoostingClassifier\n",
    "print(\"Reshaping features...\")\n",
    "training_features = training_features.reshape(training_features.shape[0], -1)\n",
    "validation_features = validation_features.reshape(validation_features.shape[0], -1)\n",
    "\n",
    "# Flatten labels to 1D\n",
    "training_labels = training_labels.flatten()\n",
    "validation_labels = val_labels.flatten()\n",
    "\n",
    "# Train Gradient Boosting Machine\n",
    "print(\"Training Gradient Boosting Machine...\")\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba1673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping features and labels...\n",
      "Evaluating on validation data...\n",
      "Validation Accuracy: 0.7938\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80     16399\n",
      "           1       0.81      0.77      0.79     16369\n",
      "\n",
      "    accuracy                           0.79     32768\n",
      "   macro avg       0.79      0.79      0.79     32768\n",
      "weighted avg       0.79      0.79      0.79     32768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure proper reshaping for training features and labels\n",
    "print(\"Reshaping features and labels...\")\n",
    "training_features = training_features.reshape(training_features.shape[0], -1)\n",
    "validation_features = validation_features.reshape(validation_features.shape[0], -1)\n",
    "\n",
    "# Flatten the labels to 1D\n",
    "training_labels_sampled = training_labels_sampled.flatten()\n",
    "validation_labels = validation_labels.flatten()\n",
    "\n",
    "gbm.fit(training_features, training_labels_sampled)\n",
    "\n",
    "# Predict on Validation Data\n",
    "print(\"Evaluating on validation data...\")\n",
    "validation_preds = gbm.predict(validation_features)\n",
    "accuracy = accuracy_score(validation_labels, validation_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(validation_labels, validation_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c311d1",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machine w/ Macenko Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae6d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_macenko_normalization(image, reference_image):\n",
    "    \"\"\"\n",
    "    Apply Macenko normalization to a single image with error handling.\n",
    "    \n",
    "    Parameters:\n",
    "        image (np.ndarray): The image to normalize, shape (H, W, C) in RGB format.\n",
    "        reference_image (np.ndarray): The reference image for normalization, shape (H, W, C) in RGB format.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The normalized image, shape (C, H, W) in normalized format.\n",
    "        None: If normalization fails for any reason.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up the transformation\n",
    "        T = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x * 255)\n",
    "        ])\n",
    "\n",
    "        # Initialize the MacenkoNormalizer\n",
    "        normalizer = torchstain.normalizers.MacenkoNormalizer(backend='torch')\n",
    "\n",
    "        # Fit the normalizer with the reference image\n",
    "        normalizer.fit(T(reference_image))\n",
    "\n",
    "        # Transform the image and apply normalization\n",
    "        t_to_transform = T(image)\n",
    "        norm_img, _, _ = normalizer.normalize(I=t_to_transform, stains=True)\n",
    "\n",
    "        # Return the normalized image\n",
    "        return norm_img.numpy()\n",
    "\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        print(f\"LinAlgError during normalization: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during normalization: {e}\")\n",
    "\n",
    "    # Return None if normalization fails\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67780ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Number of successfully preprocessed images: 2621\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Functions\n",
    "def normalize_pixel_values(image):\n",
    "    \"\"\"Normalize pixel values to [0, 1].\"\"\"\n",
    "    return image / 255.0\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Apply preprocessing to a single image.\"\"\"\n",
    "    image = apply_macenko_normalization(image, reference_image) # Stain normalization\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = normalize_pixel_values(image)  # Normalize pixel values\n",
    "    return image\n",
    "\n",
    "# Preprocess Images\n",
    "print(\"Preprocessing training data...\")\n",
    "training_data_preprocessed = []\n",
    "for img in training_data_sampled:\n",
    "    preprocessed_img = preprocess_image(img)\n",
    "    if preprocessed_img is not None:\n",
    "        training_data_preprocessed.append(preprocessed_img)\n",
    "\n",
    "# Convert the list of preprocessed images to a NumPy array\n",
    "training_data_preprocessed = np.array(training_data_preprocessed)\n",
    "\n",
    "print(f\"Number of successfully preprocessed images: {len(training_data_preprocessed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a567f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from training data...\n",
      "Preprocessing validation data...\n",
      "LinAlgError during normalization: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 2).\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "LinAlgError during normalization: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 2).\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "LinAlgError during normalization: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 2).\n",
      "LinAlgError during normalization: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 2).\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "LinAlgError during normalization: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 2).\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "LinAlgError during normalization: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 2).\n",
      "Unexpected error during normalization: kthvalue(): Expected reduction dim 0 to have non-zero size.\n",
      "Number of successfully preprocessed validation images: 32715\n",
      "Number of failed validation images: 53\n",
      "Extracting features from validation data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from training data...\")\n",
    "training_features = extract_features(training_data_preprocessed)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "# Preprocess each image and filter out None\n",
    "validation_data_preprocessed = []\n",
    "failed_indices = []  # To log indices of failed images\n",
    "for idx, img in enumerate(val_data):\n",
    "    preprocessed_img = preprocess_image(img)\n",
    "    if preprocessed_img is not None:\n",
    "        validation_data_preprocessed.append(preprocessed_img)\n",
    "    else:\n",
    "        failed_indices.append(idx)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "validation_data_preprocessed = np.array(validation_data_preprocessed)\n",
    "\n",
    "print(f\"Number of successfully preprocessed validation images: {len(validation_data_preprocessed)}\")\n",
    "print(f\"Number of failed validation images: {len(failed_indices)}\")\n",
    "\n",
    "print(\"Extracting features from validation data...\")\n",
    "validation_features = extract_features(validation_data_preprocessed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7174559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping features...\n",
      "Training Gradient Boosting Machine...\n",
      "Reshaping features and labels...\n",
      "Evaluating on validation data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [32768, 32715]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9763/2144074497.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating on validation data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mvalidation_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Accuracy: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classification Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace_and_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \"\"\"\n\u001b[1;32m    102\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [32768, 32715]"
     ]
    }
   ],
   "source": [
    "# Reshape features to 2D for compatibility with GradientBoostingClassifier\n",
    "print(\"Reshaping features...\")\n",
    "training_features = training_features.reshape(training_features.shape[0], -1)\n",
    "validation_features = validation_features.reshape(validation_features.shape[0], -1)\n",
    "\n",
    "# Flatten labels to 1D\n",
    "training_labels = training_labels.flatten()\n",
    "validation_labels = val_labels.flatten()\n",
    "\n",
    "# Train Gradient Boosting Machine\n",
    "print(\"Training Gradient Boosting Machine...\")\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Ensure proper reshaping for training features and labels\n",
    "print(\"Reshaping features and labels...\")\n",
    "training_features = training_features.reshape(training_features.shape[0], -1)\n",
    "validation_features = validation_features.reshape(validation_features.shape[0], -1)\n",
    "\n",
    "# Flatten the labels to 1D\n",
    "training_labels_sampled = training_labels_sampled.flatten()\n",
    "validation_labels = validation_labels.flatten()\n",
    "\n",
    "gbm.fit(training_features, training_labels_sampled)\n",
    "\n",
    "# Predict on Validation Data\n",
    "print(\"Evaluating on validation data...\")\n",
    "validation_preds = gbm.predict(validation_features)\n",
    "accuracy = accuracy_score(validation_labels, validation_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(validation_labels, validation_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a81c64ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7913\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80     16346\n",
      "           1       0.81      0.76      0.78     16369\n",
      "\n",
      "    accuracy                           0.79     32715\n",
      "   macro avg       0.79      0.79      0.79     32715\n",
      "weighted avg       0.79      0.79      0.79     32715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_val_labels = np.delete(validation_labels, failed_indices)\n",
    "accuracy = accuracy_score(filtered_val_labels, validation_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(filtered_val_labels, validation_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d22ac",
   "metadata": {},
   "source": [
    "### CLAHE + Reinhard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8139acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clahe(image):\n",
    "    \"\"\"Apply CLAHE (Adaptive Histogram Equalization) to enhance contrast.\"\"\"\n",
    "    # Convert to LAB color space\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l_channel, a, b = cv2.split(lab_image)\n",
    "\n",
    "    # Apply CLAHE to the L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l_channel = clahe.apply(l_channel)\n",
    "\n",
    "    # Merge and convert back to RGB\n",
    "    lab_image = cv2.merge((l_channel, a, b))\n",
    "    return cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2558842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing Functions\n",
    "def normalize_pixel_values(image):\n",
    "    \"\"\"Normalize pixel values to [0, 1].\"\"\"\n",
    "    return image / 255.0\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Apply preprocessing to a single image.\"\"\"\n",
    "    image = apply_reinhard_normalization(image, reference_image) # Stain normalization\n",
    "    image = apply_clahe(image)\n",
    "    image = normalize_pixel_values(image)  # Normalize pixel values\n",
    "    return image\n",
    "\n",
    "# Preprocess Images\n",
    "print(\"Preprocessing training data...\")\n",
    "training_data_preprocessed = np.array([preprocess_image(img) for img in training_data_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c750aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/d/dfang/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/u/home/d/dfang/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation data...\n",
      "Extracting features from validation data...\n",
      "Reshaping features...\n",
      "Training Gradient Boosting Machine...\n",
      "Reshaping features and labels...\n",
      "Evaluating on validation data...\n",
      "Validation Accuracy: 0.7993\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80     16399\n",
      "           1       0.80      0.79      0.80     16369\n",
      "\n",
      "    accuracy                           0.80     32768\n",
      "   macro avg       0.80      0.80      0.80     32768\n",
      "weighted avg       0.80      0.80      0.80     32768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from training data...\")\n",
    "training_features = extract_features(training_data_preprocessed)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "validation_data_preprocessed = np.array([preprocess_image(img) for img in val_data])\n",
    "\n",
    "print(\"Extracting features from validation data...\")\n",
    "validation_features = extract_features(validation_data_preprocessed)\n",
    "\n",
    "# Reshape features to 2D for compatibility with GradientBoostingClassifier\n",
    "print(\"Reshaping features...\")\n",
    "training_features = training_features.reshape(training_features.shape[0], -1)\n",
    "validation_features = validation_features.reshape(validation_features.shape[0], -1)\n",
    "\n",
    "# Flatten labels to 1D\n",
    "training_labels = training_labels.flatten()\n",
    "validation_labels = val_labels.flatten()\n",
    "\n",
    "# Train Gradient Boosting Machine\n",
    "print(\"Training Gradient Boosting Machine...\")\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "# Ensure proper reshaping for training features and labels\n",
    "print(\"Reshaping features and labels...\")\n",
    "training_features = training_features.reshape(training_features.shape[0], -1)\n",
    "validation_features = validation_features.reshape(validation_features.shape[0], -1)\n",
    "\n",
    "# Flatten the labels to 1D\n",
    "training_labels_sampled = training_labels_sampled.flatten()\n",
    "validation_labels = validation_labels.flatten()\n",
    "\n",
    "gbm.fit(training_features, training_labels_sampled)\n",
    "\n",
    "# Predict on Validation Data\n",
    "print(\"Evaluating on validation data...\")\n",
    "validation_preds = gbm.predict(validation_features)\n",
    "accuracy = accuracy_score(validation_labels, validation_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(validation_labels, validation_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47681b2",
   "metadata": {},
   "source": [
    "#### Tune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0fe2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation data...\n",
      "Validation Accuracy: 0.8112\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81     16399\n",
      "           1       0.82      0.80      0.81     16369\n",
      "\n",
      "    accuracy                           0.81     32768\n",
      "   macro avg       0.81      0.81      0.81     32768\n",
      "weighted avg       0.81      0.81      0.81     32768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "random_search = RandomizedSearchCV(estimator=gbm, param_distributions=param_distributions, n_iter=20, cv=3, scoring='accuracy', random_state=42)\n",
    "random_search.fit(training_features, training_labels_sampled)\n",
    "\n",
    "# Predict on Validation Data\n",
    "print(\"Evaluating on validation data...\")\n",
    "validation_preds = random_search.predict(validation_features)\n",
    "accuracy = accuracy_score(validation_labels, validation_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(validation_labels, validation_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f54757bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.8, 'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 7, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
