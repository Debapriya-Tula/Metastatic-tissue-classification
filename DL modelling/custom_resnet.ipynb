{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "\n",
    "import skimage\n",
    "from skimage.color import rgb2hed, hed2rgb\n",
    "import pywt\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, image_file, label_file, transform=None):\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load data from the H5 file\n",
    "        with h5py.File(image_file, 'r') as f:\n",
    "            self.images = f['x'][:]\n",
    "        with h5py.File(label_file, 'r') as f:\n",
    "            self.labels = f['y'][:].reshape(-1,)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2HED(torch.nn.Module):\n",
    "    def __init__(self, mode=None):\n",
    "        super(RGB2HED, self).__init__()\n",
    "        self.mode = mode\n",
    "    def forward(self, img):\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        hed_img = rgb2hed(img) * 255.\n",
    "        hed_img = np.tile(hed_img[:, :, -2:-1], reps=(1,1,3))\n",
    "        return hed_img\n",
    "    \n",
    "        \n",
    "class WaveletTransform(nn.Module):\n",
    "    def __init__(self, wavelet='haar', threshold=20):\n",
    "        super(WaveletTransform, self).__init__()\n",
    "        self.wavelet = wavelet\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def forward(self, img):\n",
    "        grayscale_image = np.dot(img.astype(np.uint8), [0.299, 0.587, 0.114])\n",
    "        \n",
    "        # Step 2: Perform 2D wavelet decomposition\n",
    "        coeffs = pywt.wavedec2(grayscale_image, wavelet=self.wavelet, level=2)\n",
    "        cA, details = coeffs[0], coeffs[1:]\n",
    "        \n",
    "        # Step 3: Apply thresholding to detail coefficients\n",
    "        def threshold_coeffs(coeffs, threshold):\n",
    "            return [pywt.threshold(c, threshold, mode='soft') for c in coeffs] \n",
    "        \n",
    "        \n",
    "        details_thresh = [threshold_coeffs(detail, self.threshold) for detail in details]\n",
    "        coeffs_thresh = [cA] + details_thresh\n",
    "        \n",
    "        # Step 4: Reconstruct the image\n",
    "        compressed_image = pywt.waverec2(coeffs_thresh, wavelet=self.wavelet)\n",
    "        compressed_image = np.clip(compressed_image, 0, 255).astype(np.uint8)\n",
    "        compressed_image = np.tile(np.expand_dims(compressed_image, -1), (1,1,3))\n",
    "        \n",
    "        return compressed_image\n",
    "    \n",
    "\n",
    "class CLAHE(nn.Module):\n",
    "    def __init__(self, mode=None):\n",
    "        super(CLAHE, self).__init__()\n",
    "        self.mode = mode\n",
    "    def forward(self, image):\n",
    "        # Convert to LAB color space\n",
    "        lab_image = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2LAB)\n",
    "        l_channel, a, b = cv2.split(lab_image)\n",
    "\n",
    "        # Apply CLAHE to the L channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        l_channel = clahe.apply(l_channel)\n",
    "\n",
    "        # Merge and convert back to RGB\n",
    "        lab_image = cv2.merge((l_channel, a, b))\n",
    "        return cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "\n",
    "class Macenko(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Macenko, self).__init__()\n",
    "    \n",
    "    def forward(self, image):\n",
    "        \"\"\"Normalize H&E stained images using Macenko method.\"\"\"\n",
    "        # Reshape image to 2D\n",
    "        h, w, c = image.shape\n",
    "        image_flat = image.reshape((-1, c))\n",
    "\n",
    "        # PCA for stain separation\n",
    "        pca = PCA(n_components=c)\n",
    "        pca.fit(image_flat)\n",
    "        stains = pca.components_\n",
    "\n",
    "        # Normalize to intensity ranges\n",
    "        norms = np.sqrt(np.sum(stains**2, axis=0))\n",
    "        normalized_stains = stains / norms\n",
    "        normalized_image = np.dot(image_flat, normalized_stains.T)\n",
    "        \n",
    "        # Scale back and reshape\n",
    "        return normalized_image.reshape((h, w, c))\n",
    "    \n",
    "class Opening(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Opening, self).__init__()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return skimage.morphology.opening(image)\n",
    "\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     # Opening(),\n",
    "#     # CLAHE(),\n",
    "#     # Macenko(),\n",
    "#     # WaveletTransform(),\n",
    "#     transforms.ToPILImage(),\n",
    "#     # transforms.Resize((96, 96)),\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                    transforms.ColorJitter(brightness=.5, saturation=.25,\n",
    "                                        hue=.1, contrast=.5),\n",
    "                    transforms.RandomAffine(10, (0.05, 0.05), fill=255),\n",
    "                    transforms.RandomHorizontalFlip(.5),\n",
    "                    transforms.RandomVerticalFlip(.5),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.6716241, 0.48636872, 0.60884315],\n",
    "                                        [0.27210504, 0.31001145, 0.2918652])])\n",
    "val_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize([0.6716241, 0.48636872, 0.60884315],\n",
    "                                                                      [0.27210504, 0.31001145, 0.2918652])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = H5Dataset(image_file='../../pcam/training_split.h5', \n",
    "                          label_file='../../Labels/Labels/camelyonpatch_level_2_split_train_y.h5', \n",
    "                          transform=transform)\n",
    "val_dataset = H5Dataset(image_file='../../pcam/validation_split.h5', \n",
    "                        label_file='../../Labels/Labels/camelyonpatch_level_2_split_valid_y.h5',\n",
    "                        transform=val_transform)\n",
    "\n",
    "test_dataset = H5Dataset(image_file='../../pcam/test_split.h5', \n",
    "                        label_file='../../Labels/Labels/camelyonpatch_level_2_split_test_y.h5',\n",
    "                        transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "bs = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "\n",
    "\n",
    "# from torch._jit_internal import weak_module, weak_script_method\n",
    "\n",
    "\n",
    "# @weak_module\n",
    "class PolarConvNd(torch.nn.modules.conv._ConvNd):\n",
    "    def __init__(self, in_channels=1, out_channels=1, kernel_size=3, dimensions=2, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
    "        self.init_kernel_size = kernel_size\n",
    "        assert kernel_size % 2 == 1, 'expected kernel size to be odd, found %d' % kernel_size\n",
    "        self.init_dimensions = dimensions\n",
    "\n",
    "        self.base_vectors = torch.from_numpy(self.build_base_vectors()).float().to(device)\n",
    "        self.true_base_vectors_shape = self.base_vectors.shape\n",
    "        self.base_vectors = self.base_vectors.view(self.true_base_vectors_shape[0],\n",
    "                                                   np.prod(self.true_base_vectors_shape[1:]).astype(int))\n",
    "\n",
    "        inferred_kernel_size = self.true_base_vectors_shape[0]\n",
    "        _kernel_size = _single(inferred_kernel_size)\n",
    "        _stride = _single(stride)\n",
    "        _padding = _single(padding)\n",
    "        _dilation = _single(dilation)\n",
    "        super(PolarConvNd, self).__init__(\n",
    "            in_channels, out_channels, _kernel_size, _stride, _padding, _dilation,\n",
    "            False, _single(0), groups, bias, padding_mode)\n",
    "\n",
    "        if dimensions == 2:\n",
    "            self.reconstructed_stride = _pair(stride)\n",
    "            self.reconstructed_padding = _pair(padding)\n",
    "            self.reconstructed_dilation = _pair(dilation)\n",
    "            self.reconstructed_conv_op = F.conv2d\n",
    "        elif dimensions == 3:\n",
    "            self.reconstructed_stride = _triple(stride)\n",
    "            self.reconstructed_padding = _triple(padding)\n",
    "            self.reconstructed_dilation = _triple(dilation)\n",
    "            self.reconstructed_conv_op = F.conv3d\n",
    "        else:\n",
    "            raise ValueError('dimension %d not supported' % dimensions)\n",
    "\n",
    "    def build_base_vectors(self):\n",
    "        kernel_size = self.init_kernel_size\n",
    "        middle = kernel_size // 2\n",
    "        dimensions = self.init_dimensions\n",
    "\n",
    "        base_vectors = []\n",
    "        # Burning phase: determine the number of base vectors\n",
    "        unique_distances = []\n",
    "        if dimensions == 2:\n",
    "            for i in range(kernel_size):\n",
    "                for j in range(kernel_size):\n",
    "                    i_ = abs(i - middle)\n",
    "                    j_ = abs(j - middle)\n",
    "                    unique_distances.append(int(i_ * i_ + j_ * j_))\n",
    "        elif dimensions == 3:\n",
    "            for i in range(kernel_size):\n",
    "                for j in range(kernel_size):\n",
    "                    for k in range(kernel_size):\n",
    "                        i_ = abs(i - middle)\n",
    "                        j_ = abs(j - middle)\n",
    "                        k_ = abs(k - middle)\n",
    "                        unique_distances.append(int(i_ * i_ + j_ * j_ + k_ * k_))\n",
    "        unique_distances, distances_counts = np.unique(unique_distances, return_counts=True)\n",
    "        unique_distances = np.sort(unique_distances)\n",
    "        print(*zip(unique_distances, distances_counts), len(unique_distances))\n",
    "\n",
    "        for unique_distance, n in zip(unique_distances, distances_counts):  # number of base vectors\n",
    "            base_vector = np.zeros([kernel_size] * dimensions)\n",
    "            if dimensions == 2:\n",
    "                for i in range(kernel_size):\n",
    "                    for j in range(kernel_size):\n",
    "                        i_ = abs(i - middle)\n",
    "                        j_ = abs(j - middle)\n",
    "                        if int(i_ * i_ + j_ * j_) == unique_distance:\n",
    "                            base_vector[i, j] = 1./n\n",
    "            elif dimensions == 3:\n",
    "                for i in range(kernel_size):\n",
    "                    for j in range(kernel_size):\n",
    "                        for k in range(kernel_size):\n",
    "                            i_ = abs(i - middle)\n",
    "                            j_ = abs(j - middle)\n",
    "                            k_ = abs(k - middle)\n",
    "                            if int(i_ * i_ + j_ * j_ + k_ * k_) == unique_distance:\n",
    "                                base_vector[i, j, k] = 1./n\n",
    "            base_vectors.append(base_vector)\n",
    "        base_vectors = np.asarray(base_vectors)\n",
    "        return base_vectors\n",
    "\n",
    "    # @weak_script_method\n",
    "    def forward(self, input):\n",
    "        weight_size = self.weight.shape\n",
    "        weight = torch.mm(self.weight.view(np.prod(weight_size[:-1]), weight_size[-1]), self.base_vectors) \\\n",
    "            .view(*weight_size[:-1], *self.true_base_vectors_shape[1:])\n",
    "        return self.reconstructed_conv_op(input, weight, self.bias, self.reconstructed_stride,\n",
    "                                          self.reconstructed_padding, self.reconstructed_dilation, self.groups)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('PolarConv%dd' % self.init_dimensions) + '(' + self.extra_repr() + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1, conv_type='classical', kernel_size=3):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    if conv_type.lower() == 'classical':\n",
    "        return nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                         padding=(kernel_size - 1) // 2, groups=groups, bias=False, dilation=dilation)\n",
    "    elif conv_type.lower() == 'polar':\n",
    "        return PolarConvNd(in_planes, out_planes, kernel_size=kernel_size, dimensions=2,\n",
    "                           stride=stride, padding=(kernel_size - 1) // 2, groups=groups, bias=False, dilation=dilation)\n",
    "    raise ValueError('unknow conv layer type %s' % conv_type)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, conv_type='classical', kernel_size=3):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride, conv_type=conv_type, kernel_size=kernel_size)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, conv_type=conv_type, kernel_size=kernel_size)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, conv_type='classical', kernel_size=3):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation, conv_type=conv_type, kernel_size=kernel_size)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_type, kernel_size, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(conv_type, kernel_size, block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(conv_type, kernel_size, block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(conv_type, kernel_size, block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(conv_type, kernel_size, block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, PolarConvNd):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, conv_type, kernel_size, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer,\n",
    "                            conv_type=conv_type, kernel_size=kernel_size))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer,\n",
    "                                conv_type=conv_type, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def _resnet(arch, conv_type, kernel_size, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(conv_type, kernel_size, block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(conv_type, kernel_size, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', conv_type, kernel_size, BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(conv_type, kernel_size, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', conv_type, kernel_size, BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(conv_type, kernel_size, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', conv_type, kernel_size, Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(conv_type, kernel_size, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', conv_type, kernel_size, Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(conv_type, kernel_size, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', conv_type, kernel_size, Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(conv_type, kernel_size, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNeXt-50 32x4d model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', conv_type, kernel_size, Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(conv_type, kernel_size, pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNeXt-101 32x8d model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', conv_type, kernel_size, Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "polar = False\n",
    "model = resnet50(conv_type='classical' if not polar else 'polar',\n",
    "                                                         kernel_size=3,\n",
    "                                                         num_classes=1)\n",
    "model = model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.functional.nll_loss\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.0, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mtest_correct\u001b[38;5;241m/\u001b[39mtest_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Train and validate the model\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/debo/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/debo/lib/python3.11/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/envs/debo/lib/python3.11/site-packages/torch/autograd/__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 220\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loops\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            train_loss += loss.item()\n",
    "            train_correct = ((outputs > .5).float() == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            # train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Metrics\n",
    "                val_loss += loss.item()\n",
    "                val_correct = ((outputs > .5).float() == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                # val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "        # Test phase\n",
    "        model.eval()\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Metrics\n",
    "                test_loss += loss.item()\n",
    "                test_correct = ((outputs > .5).float() == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                # test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {100 * train_correct/train_total:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {100 * val_correct/val_total:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {100 * test_correct/test_total:.2f}%\\n\\n\")\n",
    "\n",
    "# Train and validate the model\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
