{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import skimage\n",
    "from skimage.color import rgb2hed\n",
    "import pywt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchstain\n",
    "\n",
    "from histomicstk.preprocessing.color_normalization.deconvolution_based_normalization import (\n",
    "    deconvolution_based_normalization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, image_file, label_file, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load data from the H5 file\n",
    "        with h5py.File(image_file, \"r\") as f:\n",
    "            self.images = f[\"x\"][:]\n",
    "        with h5py.File(label_file, \"r\") as f:\n",
    "            self.labels = f[\"y\"][:].reshape(\n",
    "                -1,\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2HED(torch.nn.Module):\n",
    "    def __init__(self, mode=None):\n",
    "        super(RGB2HED, self).__init__()\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, img):\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        hed_img = rgb2hed(img) * 255.0\n",
    "        hed_img = np.tile(hed_img[:, :, -2:-1], reps=(1, 1, 3))\n",
    "        return hed_img\n",
    "\n",
    "\n",
    "class WaveletTransform(nn.Module):\n",
    "    def __init__(self, wavelet=\"haar\", threshold=20):\n",
    "        super(WaveletTransform, self).__init__()\n",
    "        self.wavelet = wavelet\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, img):\n",
    "        grayscale_image = np.dot(img.astype(np.uint8), [0.299, 0.587, 0.114])\n",
    "\n",
    "        # Step 2: Perform 2D wavelet decomposition\n",
    "        coeffs = pywt.wavedec2(grayscale_image, wavelet=self.wavelet, level=2)\n",
    "        cA, details = coeffs[0], coeffs[1:]\n",
    "\n",
    "        # Step 3: Apply thresholding to detail coefficients\n",
    "        def threshold_coeffs(coeffs, threshold):\n",
    "            return [pywt.threshold(c, threshold, mode=\"soft\") for c in coeffs]\n",
    "\n",
    "        details_thresh = [\n",
    "            threshold_coeffs(detail, self.threshold) for detail in details\n",
    "        ]\n",
    "        coeffs_thresh = [cA] + details_thresh\n",
    "\n",
    "        # Step 4: Reconstruct the image\n",
    "        compressed_image = pywt.waverec2(coeffs_thresh, wavelet=self.wavelet)\n",
    "        compressed_image = np.clip(compressed_image, 0, 255).astype(np.uint8)\n",
    "        compressed_image = np.tile(np.expand_dims(compressed_image, -1), (1, 1, 3))\n",
    "\n",
    "        return compressed_image\n",
    "\n",
    "\n",
    "class CLAHE(nn.Module):\n",
    "    def __init__(self, mode=None):\n",
    "        super(CLAHE, self).__init__()\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Convert to LAB color space\n",
    "        lab_image = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2LAB)\n",
    "        l_channel, a, b = cv2.split(lab_image)\n",
    "\n",
    "        # Apply CLAHE to the L channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        l_channel = clahe.apply(l_channel)\n",
    "\n",
    "        # Merge and convert back to RGB\n",
    "        lab_image = cv2.merge((l_channel, a, b))\n",
    "        return cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "\n",
    "class Opening(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Opening, self).__init__()\n",
    "\n",
    "    def forward(self, image):\n",
    "        return skimage.morphology.opening(image)\n",
    "\n",
    "\n",
    "class Macenko(nn.Module):\n",
    "    def __init__(self, reference_image, target_W=None, alpha=1, beta=0.01):\n",
    "        super(Macenko, self).__init__()\n",
    "        self.target_W = target_W\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.reference_image = reference_image.astype(np.uint8)\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Apply Macenko normalization to a single image with error handling.\n",
    "\n",
    "        Parameters:\n",
    "            image (np.ndarray): The image to normalize, shape (H, W, C) in RGB format.\n",
    "            reference_image (np.ndarray): The reference image for normalization, shape (H, W, C) in RGB format.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The normalized image, shape (C, H, W) in normalized format.\n",
    "            None: If normalization fails for any reason.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # # Set up the transformation\n",
    "            # T = transforms.Compose([\n",
    "            #     transforms.ToTensor(),\n",
    "            # ])\n",
    "\n",
    "            # Initialize the MacenkoNormalizer\n",
    "            normalizer = torchstain.normalizers.MacenkoNormalizer(backend=\"torch\")\n",
    "\n",
    "            # Fit the normalizer with the reference image\n",
    "            normalizer.fit(self.reference_image)\n",
    "\n",
    "            # Transform the image and apply normalization\n",
    "            t_to_transform = image\n",
    "            norm_img, _, _ = normalizer.normalize(I=t_to_transform, stains=True)\n",
    "            print('norm_img', norm_img.shape)\n",
    "            final_img = torch.stack((image, norm_img), axis=-3)\n",
    "            # print('final img', final_img.shape)\n",
    "\n",
    "            # Return the normalized image\n",
    "            return final_img\n",
    "\n",
    "        except torch.linalg.LinAlgError as e:\n",
    "            # print(f\"LinAlgError during normalization: {e}\")\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            # print(f\"Unexpected error during normalization: {e}\")\n",
    "            pass\n",
    "\n",
    "        # Return None if normalization fails\n",
    "        return torch.stack((image, image), axis=-3)\n",
    "\n",
    "\n",
    "class ReinhardNormalization(nn.Module):\n",
    "    def __init__(self, reference_image):\n",
    "        super(ReinhardNormalization, self).__init__()\n",
    "        self.reference_image = reference_image.astype(np.uint8)\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Apply Reinhard normalization to a single image with error handling.\n",
    "\n",
    "        Parameters:\n",
    "            image (np.ndarray): The image to normalize, shape (H, W, C) in RGB format.\n",
    "            reference_image (np.ndarray): The reference image for normalization, shape (H, W, C) in RGB format.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The normalized image, shape (H, W, C) in normalized format.\n",
    "            None: If normalization fails for any reason.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize the ReinhardNormalizer\n",
    "            normalizer = torchstain.normalizers.ReinhardNormalizer()\n",
    "\n",
    "            # Fit the normalizer with the reference image\n",
    "            normalizer.fit(self.reference_image)\n",
    "\n",
    "            # Normalize the image\n",
    "            normalized_image = normalizer.normalize(image)\n",
    "            final_img = torch.stack((image, normalized_image), axis=-3)\n",
    "            print('final img', final_img.shape)\n",
    "\n",
    "            # Return the normalized image\n",
    "            return normalized_image\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Unexpected error during Reinhard normalization: {e}\")\n",
    "            pass\n",
    "\n",
    "        # Return None if normalization fails\n",
    "        return image #torch.stack((image, image), axis=-3)\n",
    "\n",
    "\n",
    "train_data = H5Dataset(\n",
    "    image_file=\"../../../pcam/training_split.h5\",\n",
    "    label_file=\"../../../Labels/Labels/camelyonpatch_level_2_split_train_y.h5\",\n",
    ")\n",
    "reference_image = train_data.images[176298]\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        # RGB2HED(),\n",
    "        # WaveletTransform(),\n",
    "        # CLAHE(),\n",
    "        # Opening(),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(brightness=0.5, saturation=0.25, hue=0.1, contrast=0.5),\n",
    "        transforms.RandomAffine(10, (0.05, 0.05), fill=255),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomVerticalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        Macenko(reference_image=reference_image),\n",
    "        # ReinhardNormalization(reference_image=reference_image),\n",
    "        # transforms.Normalize(\n",
    "        #     [0.6716241, 0.48636872, 0.60884315, 0.6716241, 0.48636872, 0.60884315],\n",
    "        #     [0.27210504, 0.31001145, 0.2918652, 0.6716241, 0.48636872, 0.60884315]\n",
    "        # ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(\n",
    "        #     [0.6716241, 0.48636872, 0.60884315, 0.6716241, 0.48636872, 0.60884315], \n",
    "        #     [0.27210504, 0.31001145, 0.2918652, 0.6716241, 0.48636872, 0.60884315]\n",
    "        # ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = H5Dataset(\n",
    "    image_file=\"../../../pcam/training_split.h5\",\n",
    "    label_file=\"../../../Labels/Labels/camelyonpatch_level_2_split_train_y.h5\",\n",
    "    transform=train_transform,\n",
    ")\n",
    "val_dataset = H5Dataset(\n",
    "    image_file=\"../../../pcam/validation_split.h5\",\n",
    "    label_file=\"../../../Labels/Labels/camelyonpatch_level_2_split_valid_y.h5\",\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = H5Dataset(\n",
    "    image_file=\"../../../pcam/test_split.h5\",\n",
    "    label_file=\"../../../Labels/Labels/camelyonpatch_level_2_split_test_y.h5\",\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "bs = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.resnet = resnet50(pretrained=False)\n",
    "        # Replace the final fully connected layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetModel(num_classes=2).to(device)  # Binary classification\n",
    "model.resnet.conv1.in_channels = 6\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_921154/1861484373.py\", line 21, in __getitem__\n    image = self.transform(image)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 277, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 350, in normalize\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py\", line 928, in normalize\n    return tensor.sub_(mean).div_(std)\n           ^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 75\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mtest_correct\u001b[38;5;241m/\u001b[39mtest_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Train and validate the model\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m train_loss, train_correct, train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/debo/lib/python3.11/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_921154/1861484373.py\", line 21, in __getitem__\n    image = self.transform(image)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 277, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 350, in normalize\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vaibhav/miniconda3/envs/debo/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py\", line 928, in normalize\n    return tensor.sub_(mean).div_(std)\n           ^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loops\n",
    "def train_and_validate(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs=10\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Metrics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Test phase\n",
    "        model.eval()\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Metrics\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {100 * train_correct/train_total:.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {100 * val_correct/val_total:.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {100 * test_correct/test_total:.2f}%\\n\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Train and validate the model\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adamw lr=1e-3, wd=0.1\n",
    "\n",
    "# Epoch 1/20\n",
    "# Train Loss: 0.4875, Train Acc: 77.33%\n",
    "# Val Loss: 0.5107, Val Acc: 76.40%\n",
    "# Test Loss: 0.5035, Test Acc: 77.21%\n",
    "\n",
    "\n",
    "# Epoch 2/20\n",
    "# Train Loss: 0.3851, Train Acc: 83.07%\n",
    "# Val Loss: 0.3695, Val Acc: 83.42%\n",
    "# Test Loss: 0.3493, Test Acc: 84.66%\n",
    "\n",
    "\n",
    "# Epoch 3/20\n",
    "# Train Loss: 0.3143, Train Acc: 86.64%\n",
    "# Val Loss: 0.3746, Val Acc: 83.56%\n",
    "# Test Loss: 0.3669, Test Acc: 82.75%\n",
    "\n",
    "\n",
    "# Epoch 4/20\n",
    "# Train Loss: 0.2835, Train Acc: 88.26%\n",
    "# Val Loss: 0.3393, Val Acc: 85.56%\n",
    "# Test Loss: 0.3460, Test Acc: 84.50%\n",
    "\n",
    "\n",
    "# Epoch 5/20\n",
    "# Train Loss: 0.2653, Train Acc: 89.18%\n",
    "# Val Loss: 0.4198, Val Acc: 84.33%\n",
    "# Test Loss: 0.4474, Test Acc: 81.17%\n",
    "\n",
    "\n",
    "# Epoch 6/20\n",
    "# Train Loss: 0.2547, Train Acc: 89.61%\n",
    "# Val Loss: 0.3080, Val Acc: 86.96%\n",
    "# Test Loss: 0.3253, Test Acc: 85.78%\n",
    "\n",
    "\n",
    "# Epoch 7/20\n",
    "# Train Loss: 0.2447, Train Acc: 90.04%\n",
    "# Val Loss: 0.2909, Val Acc: 87.81%\n",
    "# Test Loss: 0.3318, Test Acc: 85.75%\n",
    "\n",
    "\n",
    "# Epoch 8/20\n",
    "# Train Loss: 0.2380, Train Acc: 90.41%\n",
    "# Val Loss: 0.2811, Val Acc: 88.64%\n",
    "# Test Loss: 0.3492, Test Acc: 85.32%\n",
    "\n",
    "\n",
    "# Epoch 9/20\n",
    "# Train Loss: 0.2333, Train Acc: 90.70%\n",
    "# Val Loss: 0.2742, Val Acc: 88.77%\n",
    "# Test Loss: 0.2985, Test Acc: 87.68%\n",
    "\n",
    "\n",
    "# Epoch 10/20\n",
    "# Train Loss: 0.2280, Train Acc: 90.88%\n",
    "# Val Loss: 0.2770, Val Acc: 88.74%\n",
    "# Test Loss: 0.2955, Test Acc: 87.54%\n",
    "\n",
    "\n",
    "# Epoch 11/20\n",
    "# Train Loss: 0.2250, Train Acc: 91.04%\n",
    "# Val Loss: 0.3370, Val Acc: 87.11%\n",
    "# Test Loss: 0.3484, Test Acc: 85.82%\n",
    "\n",
    "\n",
    "# Epoch 12/20\n",
    "# Train Loss: 0.2209, Train Acc: 91.23%\n",
    "# Val Loss: 0.4230, Val Acc: 81.84%\n",
    "# Test Loss: 0.4566, Test Acc: 80.81%\n",
    "\n",
    "\n",
    "# Epoch 13/20\n",
    "# Train Loss: 0.2188, Train Acc: 91.34%\n",
    "# Val Loss: 0.3244, Val Acc: 86.85%\n",
    "# Test Loss: 0.3417, Test Acc: 84.73%\n",
    "\n",
    "\n",
    "# Epoch 14/20\n",
    "# Train Loss: 0.2153, Train Acc: 91.44%\n",
    "# Val Loss: 0.3606, Val Acc: 86.03%\n",
    "# Test Loss: 0.4001, Test Acc: 82.55%\n",
    "\n",
    "\n",
    "# Epoch 15/20\n",
    "# Train Loss: 0.2147, Train Acc: 91.51%\n",
    "# Val Loss: 0.2853, Val Acc: 88.75%\n",
    "# Test Loss: 0.3511, Test Acc: 85.88%\n",
    "\n",
    "\n",
    "# Epoch 16/20\n",
    "# Train Loss: 0.2147, Train Acc: 91.51%\n",
    "# Val Loss: 0.2635, Val Acc: 89.07%\n",
    "# Test Loss: 0.3208, Test Acc: 86.80%\n",
    "\n",
    "\n",
    "# Epoch 17/20\n",
    "# Train Loss: 0.2120, Train Acc: 91.64%\n",
    "# Val Loss: 0.3522, Val Acc: 86.07%\n",
    "# Test Loss: 0.3377, Test Acc: 85.63%\n",
    "\n",
    "\n",
    "# Epoch 18/20\n",
    "# Train Loss: 0.2099, Train Acc: 91.74%\n",
    "# Val Loss: 0.2562, Val Acc: 89.83%\n",
    "# Test Loss: 0.2840, Test Acc: 88.69%\n",
    "\n",
    "\n",
    "# Epoch 19/20\n",
    "# Train Loss: 0.2096, Train Acc: 91.73%\n",
    "# Val Loss: 0.3078, Val Acc: 88.31%\n",
    "# Test Loss: 0.3197, Test Acc: 87.25%\n",
    "\n",
    "\n",
    "# Epoch 20/20\n",
    "# Train Loss: 0.2084, Train Acc: 91.76%\n",
    "# Val Loss: 0.2884, Val Acc: 87.91%\n",
    "# Test Loss: 0.2865, Test Acc: 87.74%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adamw lr=1e-4, wd=0.01\n",
    "\n",
    "# Epoch 1/10\n",
    "# Train Loss: 0.5258, Train Acc: 74.43%\n",
    "# Val Loss: 0.4653, Val Acc: 78.62%\n",
    "# Test Loss: 0.4727, Test Acc: 78.53%\n",
    "\n",
    "\n",
    "# Epoch 2/10\n",
    "# Train Loss: 0.4634, Train Acc: 78.67%\n",
    "# Val Loss: 0.4820, Val Acc: 76.75%\n",
    "# Test Loss: 0.4805, Test Acc: 77.37%\n",
    "\n",
    "\n",
    "# Epoch 3/10\n",
    "# Train Loss: 0.4302, Train Acc: 80.49%\n",
    "# Val Loss: 0.4225, Val Acc: 79.57%\n",
    "# Test Loss: 0.4196, Test Acc: 80.92%\n",
    "\n",
    "\n",
    "# Epoch 4/10\n",
    "# Train Loss: 0.3916, Train Acc: 82.53%\n",
    "# Val Loss: 0.4054, Val Acc: 80.63%\n",
    "# Test Loss: 0.3912, Test Acc: 81.98%\n",
    "\n",
    "\n",
    "# Epoch 5/10\n",
    "# Train Loss: 0.3528, Train Acc: 84.56%\n",
    "# Val Loss: 0.4083, Val Acc: 80.78%\n",
    "# Test Loss: 0.3981, Test Acc: 81.82%\n",
    "\n",
    "\n",
    "# Epoch 6/10\n",
    "# Train Loss: 0.3193, Train Acc: 86.38%\n",
    "# Val Loss: 0.3495, Val Acc: 83.68%\n",
    "# Test Loss: 0.3886, Test Acc: 82.54%\n",
    "\n",
    "\n",
    "# Epoch 7/10\n",
    "# Train Loss: 0.2909, Train Acc: 87.75%\n",
    "# Val Loss: 0.3562, Val Acc: 84.77%\n",
    "# Test Loss: 0.3520, Test Acc: 85.36%\n",
    "\n",
    "\n",
    "# Epoch 8/10\n",
    "# Train Loss: 0.2679, Train Acc: 88.99%\n",
    "# Val Loss: 0.3755, Val Acc: 84.72%\n",
    "# Test Loss: 0.4703, Test Acc: 80.82%\n",
    "\n",
    "\n",
    "# Epoch 9/10\n",
    "# Train Loss: 0.2513, Train Acc: 89.77%\n",
    "# Val Loss: 0.2995, Val Acc: 87.69%\n",
    "# Test Loss: 0.3098, Test Acc: 86.81%\n",
    "\n",
    "\n",
    "# Epoch 10/10\n",
    "# Train Loss: 0.2349, Train Acc: 90.56%\n",
    "# Val Loss: 0.3082, Val Acc: 87.26%\n",
    "# Test Loss: 0.3217, Test Acc: 86.59%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
